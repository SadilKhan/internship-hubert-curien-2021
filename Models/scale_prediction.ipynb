{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "scale_prediction.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "metadata": {
      "interpreter": {
        "hash": "7a42647e4c83c11f35b3df9f93177de0f6edb84fc1cf48b1d5ae91b569103241"
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8OyDt9vhmuw"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torchvision\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID5AwyEWkfW2",
        "outputId": "4a51db32-4fde-48a7-bd4e-335b2994894f"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMDsQrOThrEi",
        "outputId": "8cfe8d89-7bb8-40fa-e1b9-a8d17ae786b3"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jre4rm2hmuz",
        "outputId": "9d3fc9ab-3e57-46a7-e0a9-a5f9ef1ee655"
      },
      "source": [
        "PATH=\"/content/gdrive/MyDrive\"\n",
        "os.listdir(PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Colab Notebooks', 'train_images', 'csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxj6x5lehmu0"
      },
      "source": [
        "# Declaring Global Varibles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUpTuKXNhmu0"
      },
      "source": [
        "CSV_PATH=PATH+\"/csv\"\n",
        "IMAGE_PATH=PATH+\"/train_images\"\n",
        "BS=8\n",
        "LR=1e-2\n",
        "WEIGHT_DECAY=5E-4\n",
        "EPOCH=10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYO3KYumhmu1"
      },
      "source": [
        "# Importing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "n5SuB1FLhmu1",
        "outputId": "9bdb826d-bc16-42c9-c0cc-d38c0af2b86e"
      },
      "source": [
        "data=pd.read_csv(CSV_PATH+\"/train.csv\")\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>label</th>\n",
              "      <th>template_name</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0181 copie.jpg</td>\n",
              "      <td>0181 copie_0</td>\n",
              "      <td>0181 copie_0_0.jpg</td>\n",
              "      <td>216</td>\n",
              "      <td>696</td>\n",
              "      <td>545</td>\n",
              "      <td>1040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0181 copie.jpg</td>\n",
              "      <td>0181 copie_0_flipped</td>\n",
              "      <td>0181 copie_0_flipped_0.jpg</td>\n",
              "      <td>852</td>\n",
              "      <td>687</td>\n",
              "      <td>1210</td>\n",
              "      <td>1038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0181 copie.jpg</td>\n",
              "      <td>0181 copie_1</td>\n",
              "      <td>0181 copie_1_0.jpg</td>\n",
              "      <td>557</td>\n",
              "      <td>701</td>\n",
              "      <td>861</td>\n",
              "      <td>1080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0181 copie.jpg</td>\n",
              "      <td>0181 copie_2</td>\n",
              "      <td>0181 copie_2_0.jpg</td>\n",
              "      <td>194</td>\n",
              "      <td>1201</td>\n",
              "      <td>419</td>\n",
              "      <td>1380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0181 copie.jpg</td>\n",
              "      <td>0181 copie_2</td>\n",
              "      <td>0181 copie_2_1.jpg</td>\n",
              "      <td>1019</td>\n",
              "      <td>1201</td>\n",
              "      <td>1275</td>\n",
              "      <td>1379</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       image_name                 label  ...  xmax  ymax\n",
              "0  0181 copie.jpg          0181 copie_0  ...   545  1040\n",
              "1  0181 copie.jpg  0181 copie_0_flipped  ...  1210  1038\n",
              "2  0181 copie.jpg          0181 copie_1  ...   861  1080\n",
              "3  0181 copie.jpg          0181 copie_2  ...   419  1380\n",
              "4  0181 copie.jpg          0181 copie_2  ...  1275  1379\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-q_yEfRhmu2",
        "outputId": "148ec731-04cd-4754-8b34-0b2b017cc5a6"
      },
      "source": [
        "image_names=data['image_name'].unique()\n",
        "print(f\"We have {len(image_names)} images\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 18 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbRGIEHmhmu2"
      },
      "source": [
        "# Store the arrays of the image\n",
        "image_names=list(data['image_name'].unique())\n",
        "imageArray=pd.DataFrame({\"image_name\":[],\"image_array\":[]})\n",
        "for im in image_names:\n",
        "  img=Image.open(IMAGE_PATH+\"/\"+im)\n",
        "  imageArray=pd.concat([imageArray,pd.DataFrame({\"image_name\":[im],\"image_array\":[np.array(img)]})])\n",
        "  del img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Mcp3xykchmu2",
        "outputId": "f8575db7-36c7-4944-dcbc-b2378f0bb468"
      },
      "source": [
        "imageArray.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>image_array</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0181 copie.jpg</td>\n",
              "      <td>[[[145, 138, 132], [140, 133, 127], [140, 133,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0183 copie.jpg</td>\n",
              "      <td>[[[153, 146, 140], [165, 158, 152], [133, 126,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0185 copie.jpg</td>\n",
              "      <td>[[[163, 156, 148], [156, 149, 141], [151, 144,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0187 copie.jpg</td>\n",
              "      <td>[[[151, 144, 138], [151, 144, 138], [148, 141,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0189 copie.jpg</td>\n",
              "      <td>[[[143, 136, 130], [146, 139, 133], [133, 126,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       image_name                                        image_array\n",
              "0  0181 copie.jpg  [[[145, 138, 132], [140, 133, 127], [140, 133,...\n",
              "0  0183 copie.jpg  [[[153, 146, 140], [165, 158, 152], [133, 126,...\n",
              "0  0185 copie.jpg  [[[163, 156, 148], [156, 149, 141], [151, 144,...\n",
              "0  0187 copie.jpg  [[[151, 144, 138], [151, 144, 138], [148, 141,...\n",
              "0  0189 copie.jpg  [[[143, 136, 130], [146, 139, 133], [133, 126,..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm7RxbEohmu3"
      },
      "source": [
        "# Pretext Task\n",
        "\n",
        "    > Predict Scale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63UjbMFshmu3"
      },
      "source": [
        "# We won't save the arrays of the rescaled image, we will save the scaling factor and coordinates for crop\n",
        "scaleData=pd.DataFrame({\"image_name\":[],\"x_min\":[],\"y_min\":[],\"x_max\":[],\"y_max\":[],\"scaling_factor\":[]})\n",
        "scaleData[\"x_min\"]=scaleData[\"image_name\"].apply(lambda x:0)\n",
        "scaleData[\"y_min\"]=scaleData[\"image_name\"].apply(lambda x:0)\n",
        "scaleData[\"x_max\"]=scaleData[\"image_name\"].apply(lambda x:\n",
        "                                                 imageArray[imageArray[\"image_name\"]==x][\"image_array\"][0].shape[1])\n",
        "scaleData[\"y_max\"]=scaleData[\"image_name\"].apply(lambda x:\n",
        "                                                 imageArray[imageArray[\"image_name\"]==x][\"image_array\"][0].shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0JMLxvEhmu3"
      },
      "source": [
        "# We will randomly crop an image and scale it to 300 * 300 and store the scaling factor as a target variable\n",
        "scaling_factors=np.arange(1,3.1,0.1)\n",
        "def rescale_crop(num,image_name):\n",
        "    \"\"\" Randomly crop an image and scale it to 300 * 300 \"\"\"\n",
        "    global scaleData\n",
        "    global data\n",
        "    global scaling_factors\n",
        "\n",
        "    h,w=imageArray[imageArray['image_name']==image_name][\"image_array\"].values[0].shape[:2]\n",
        "\n",
        "    # Choose randomly a scaling factor\n",
        "    sc=np.random.choice(scaling_factors)\n",
        "\n",
        "    # Randomly choose a height for image between 400 to 1000\n",
        "    new_w=int(300*sc)\n",
        "    new_h=int(300*sc)\n",
        "\n",
        "    # Choose randomly the coordinates\n",
        "    new_x=int(np.random.randint(0,w-2*new_w))\n",
        "    new_y=int(np.random.randint(0,h-new_h))\n",
        "\n",
        "    # image name\n",
        "    name=image_name.split(\".\")[0]+\"_scale_\"+str(num)+\".jpg\"\n",
        "\n",
        "    scaleData=pd.concat([scaleData,pd.DataFrame({\"image_name\":[name],\"x_min\":[new_x],\n",
        "    \"y_min\":[new_y],\"x_max\":[new_x+new_w],\"y_max\":[new_y+new_h],\"scaling_factor\":[sc]})])\n",
        "     \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RG39_yMhmu4",
        "outputId": "16236af6-a6c6-41c2-c974-1dc50de8cd43"
      },
      "source": [
        "%%time\n",
        "for im in image_names:\n",
        "    for i in range(200):\n",
        "        rescale_crop(i,im)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 10.6 s, sys: 161 ms, total: 10.8 s\n",
            "Wall time: 10.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78wUzLS-hmu4"
      },
      "source": [
        "le=LabelEncoder()\n",
        "scaleData['scaling_factor']=le.fit_transform(scaleData['scaling_factor'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "KxAJHUFqhmu4",
        "outputId": "4c77e32c-6770-40d7-9e3b-440f4b73f656"
      },
      "source": [
        "scaleData.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>x_min</th>\n",
              "      <th>y_min</th>\n",
              "      <th>x_max</th>\n",
              "      <th>y_max</th>\n",
              "      <th>scaling_factor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0181 copie_scale_0.jpg</td>\n",
              "      <td>434.0</td>\n",
              "      <td>2147.0</td>\n",
              "      <td>1214.0</td>\n",
              "      <td>2927.0</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0181 copie_scale_1.jpg</td>\n",
              "      <td>597.0</td>\n",
              "      <td>1032.0</td>\n",
              "      <td>1257.0</td>\n",
              "      <td>1692.0</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0181 copie_scale_2.jpg</td>\n",
              "      <td>363.0</td>\n",
              "      <td>2415.0</td>\n",
              "      <td>993.0</td>\n",
              "      <td>3045.0</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0181 copie_scale_3.jpg</td>\n",
              "      <td>943.0</td>\n",
              "      <td>1584.0</td>\n",
              "      <td>1393.0</td>\n",
              "      <td>2034.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0181 copie_scale_4.jpg</td>\n",
              "      <td>582.0</td>\n",
              "      <td>3090.0</td>\n",
              "      <td>1212.0</td>\n",
              "      <td>3720.0</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               image_name  x_min   y_min   x_max   y_max  scaling_factor\n",
              "0  0181 copie_scale_0.jpg  434.0  2147.0  1214.0  2927.0              16\n",
              "0  0181 copie_scale_1.jpg  597.0  1032.0  1257.0  1692.0              12\n",
              "0  0181 copie_scale_2.jpg  363.0  2415.0   993.0  3045.0              11\n",
              "0  0181 copie_scale_3.jpg  943.0  1584.0  1393.0  2034.0               5\n",
              "0  0181 copie_scale_4.jpg  582.0  3090.0  1212.0  3720.0              11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmpSq8oshmu5"
      },
      "source": [
        "\n",
        "scaleData.to_csv(CSV_PATH+\"/\"+\"scaleData.csv\",index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqWaDkaYhmu6",
        "outputId": "d5c36a28-e637-404a-83a5-450e4f560e4c"
      },
      "source": [
        "imageArray[imageArray['image_name']==\"0181 copie.jpg\"]['image_array'].values[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3806, 2072, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTgF_w9Uhmu6",
        "outputId": "897ad07d-6228-497d-c075-7e5a253c4f2e"
      },
      "source": [
        "img=np.array(Image.open(IMAGE_PATH+\"/\"+\"0181 copie.jpg\"))\n",
        "template=img[1206:1746,404:944,:]\n",
        "template.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(540, 540, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGPA7coOhmu7"
      },
      "source": [
        "class ScaleDataset(Dataset):\n",
        "    def __init__(self, dataset, image_dataset, is_test=False, transform=None):\n",
        "        #self.annotation_folder_path = csv_path\n",
        "        self.dataset=dataset\n",
        "        self.image_dataset=image_dataset\n",
        "        self.all_images=self.dataset['image_name'].unique()\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        img_name=self.all_images[idx]\n",
        "        original_img_name=img_name.split(\"_\")[0]+\".jpg\"\n",
        "        coord=self.dataset[self.dataset['image_name']==img_name][[\"x_min\",\"y_min\",\"x_max\",\"y_max\"]].values[0]\n",
        "        img=Image.fromarray(self.image_dataset[self.image_dataset['image_name']==original_img_name]['image_array'].values[0][\n",
        "            int(coord[1]):int(coord[3]),int(coord[0]):int(coord[2])])\n",
        "        #img=img.convert(\"RGB\")\n",
        "\n",
        "        if not self.is_test:\n",
        "            annotations=self.dataset[self.dataset['image_name']==img_name]\n",
        "\n",
        "            #self.box = self.get_xy(annotations)\n",
        "\n",
        "            #self.new_box = torch.FloatTensor(self.box_resize(self.box, img))\n",
        "            if self.transform is not None:\n",
        "                img = self.transform(img)\n",
        "            \n",
        "\n",
        "            self.labels=torch.FloatTensor(annotations['scaling_factor'].values)\n",
        "\n",
        "            return img, self.labels\n",
        "        else:\n",
        "            return img\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.all_images)\n",
        "        \n",
        "    \n",
        "    def collate_fn(self, batch):\n",
        "        \"\"\"\n",
        "        :param batch: an iterable of N sets from __getitem__()\n",
        "        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
        "        \"\"\"\n",
        "\n",
        "        images = list()\n",
        "        labels = list()\n",
        "#         difficulties = list()\n",
        "\n",
        "        for b in batch:\n",
        "            images.append(b[0])\n",
        "            labels.append(b[1])\n",
        "#             difficulties.append(b[3])\n",
        "\n",
        "        images = torch.stack(images, dim=0)\n",
        "\n",
        "        return images, labels\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6aLY8K0hmu7"
      },
      "source": [
        "tsfm = transforms.Compose([\n",
        "    transforms.Resize([300,300]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0Tkpn2qhmu7"
      },
      "source": [
        "x_train,x_test=train_test_split(scaleData,test_size=0.15,random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EML7sjdPhmu8"
      },
      "source": [
        "train_ds = ScaleDataset(x_train,imageArray,transform=tsfm)\n",
        "train_dl = DataLoader(train_ds, batch_size=BS, shuffle=True, collate_fn=train_ds.collate_fn)\n",
        "\n",
        "valid_ds = ScaleDataset(x_test,imageArray, transform=None)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=BS, shuffle=True, collate_fn=valid_ds.collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V__XS1NDhmu8"
      },
      "source": [
        "def decimate(tensor, m):\n",
        "    \"\"\"\n",
        "    Decimate a tensor by a factor 'm', i.e. downsample by keeping every 'm'th value.\n",
        "    This is used when we convert FC layers to equivalent Convolutional layers, BUT of a smaller size.\n",
        "    :param tensor: tensor to be decimated\n",
        "    :param m: list of decimation factors for each dimension of the tensor; None if not to be decimated along a dimension\n",
        "    :return: decimated tensor\n",
        "    \"\"\"\n",
        "    assert tensor.dim() == len(m)\n",
        "    for d in range(tensor.dim()):\n",
        "        if m[d] is not None:\n",
        "            tensor = tensor.index_select(dim=d,\n",
        "                                         index=torch.arange(start=0, end=tensor.size(d), step=m[d]).long())\n",
        "\n",
        "    return tensor\n",
        "class VGGBase(nn.Module):\n",
        "    \"\"\"\n",
        "    VGG base convolutions to produce lower-level feature maps.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(VGGBase, self).__init__()\n",
        "\n",
        "        # Standard convolutional layers in VGG16\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)  # stride = 1, by default\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)  # ceiling (not floor) here for even dims\n",
        "\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # retains size because stride is 1 (and padding)\n",
        "\n",
        "        # Replacements for FC6 and FC7 in VGG16\n",
        "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)  # atrous convolution\n",
        "\n",
        "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "        # Load pretrained layers\n",
        "        self.load_pretrained_layers()\n",
        "\n",
        "        # Linear Layers\n",
        "        self.linear1=nn.Linear(in_features=1024*19*19,out_features=512)\n",
        "        self.linear2=nn.Linear(in_features=512,out_features=256)\n",
        "        self.output=nn.Linear(in_features=256,out_features=21)\n",
        "\n",
        "\n",
        "    def forward(self, image):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n",
        "        :return: lower-level feature maps conv4_3 and conv7\n",
        "        \"\"\"\n",
        "        out = F.relu(self.conv1_1(image))  # (N, 64, 300, 300)\n",
        "        out = F.relu(self.conv1_2(out))  # (N, 64, 300, 300)\n",
        "        out = self.pool1(out)  # (N, 64, 150, 150)\n",
        "\n",
        "        out = F.relu(self.conv2_1(out))  # (N, 128, 150, 150)\n",
        "        out = F.relu(self.conv2_2(out))  # (N, 128, 150, 150)\n",
        "        out = self.pool2(out)  # (N, 128, 75, 75)\n",
        "\n",
        "        out = F.relu(self.conv3_1(out))  # (N, 256, 75, 75)\n",
        "        out = F.relu(self.conv3_2(out))  # (N, 256, 75, 75)\n",
        "        out = F.relu(self.conv3_3(out))  # (N, 256, 75, 75)\n",
        "        out = self.pool3(out)  # (N, 256, 38, 38), it would have been 37 if not for ceil_mode = True\n",
        "\n",
        "        out = F.relu(self.conv4_1(out))  # (N, 512, 38, 38)\n",
        "        out = F.relu(self.conv4_2(out))  # (N, 512, 38, 38)\n",
        "        out = F.relu(self.conv4_3(out))  # (N, 512, 38, 38)\n",
        "        conv4_3_feats = out  # (N, 512, 38, 38)\n",
        "        out = self.pool4(out)  # (N, 512, 19, 19)\n",
        "\n",
        "        out = F.relu(self.conv5_1(out))  # (N, 512, 19, 19)\n",
        "        out = F.relu(self.conv5_2(out))  # (N, 512, 19, 19)\n",
        "        out = F.relu(self.conv5_3(out))  # (N, 512, 19, 19)\n",
        "        out = self.pool5(out)  # (N, 512, 19, 19), pool5 does not reduce dimensions\n",
        "\n",
        "        out = F.relu(self.conv6(out))  # (N, 1024, 19, 19)\n",
        "\n",
        "        conv7_feats = F.relu(self.conv7(out))  # (N, 1024, 19, 19)\n",
        "\n",
        "        # Linear Layers\n",
        "        flattened_array=conv7_feats.reshape(-1,1024*19*19)\n",
        "        out=F.relu(self.linear1(flattened_array))\n",
        "        out=F.relu(self.linear2(out))\n",
        "        output=F.softmax(self.output(out))\n",
        "\n",
        "        # Lower-level feature maps\n",
        "        return output\n",
        "\n",
        "    def load_pretrained_layers(self):\n",
        "        \"\"\"\n",
        "        As in the paper, we use a VGG-16 pretrained on the ImageNet task as the base network.\n",
        "        There's one available in PyTorch, see https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.vgg16\n",
        "        We copy these parameters into our network. It's straightforward for conv1 to conv5.\n",
        "        However, the original VGG-16 does not contain the conv6 and con7 layers.\n",
        "        Therefore, we convert fc6 and fc7 into convolutional layers, and subsample by decimation. See 'decimate' in utils.py.\n",
        "        \"\"\"\n",
        "        # Current state of base\n",
        "        state_dict = self.state_dict()\n",
        "        param_names = list(state_dict.keys())\n",
        "\n",
        "        # Pretrained VGG base\n",
        "        pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()\n",
        "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
        "\n",
        "        # Transfer conv. parameters from pretrained model to current model\n",
        "        for i, param in enumerate(param_names[:-4]):  # excluding conv6 and conv7 parameters\n",
        "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
        "\n",
        "        # Convert fc6, fc7 to convolutional layers, and subsample (by decimation) to sizes of conv6 and conv7\n",
        "        # fc6\n",
        "        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)  # (4096, 512, 7, 7)\n",
        "        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']  # (4096)\n",
        "        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n",
        "        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])  # (1024)\n",
        "        # fc7\n",
        "        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)  # (4096, 4096, 1, 1)\n",
        "        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']  # (4096)\n",
        "        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None])  # (1024, 1024, 1, 1)\n",
        "        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])  # (1024)\n",
        "\n",
        "        # Note: an FC layer of size (K) operating on a flattened version (C*H*W) of a 2D image of size (C, H, W)...\n",
        "        # ...is equivalent to a convolutional layer with kernel size (H, W), input channels C, output channels K...\n",
        "        # ...operating on the 2D image of size (C, H, W) without padding\n",
        "\n",
        "        self.load_state_dict(state_dict)\n",
        "\n",
        "        print(\"\\nLoaded base model.\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIIpGNPXhmu9",
        "outputId": "ec2e69ce-7a88-4c6c-d2be-87f054a551d8"
      },
      "source": [
        "vgg=VGGBase().to(\"cuda:0\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded base model.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVX-giE2hmu9",
        "outputId": "a2e81e2f-765a-48a0-f7ab-e295487da9db"
      },
      "source": [
        "summary(vgg,(3,300,300))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 300, 300]           1,792\n",
            "            Conv2d-2         [-1, 64, 300, 300]          36,928\n",
            "         MaxPool2d-3         [-1, 64, 150, 150]               0\n",
            "            Conv2d-4        [-1, 128, 150, 150]          73,856\n",
            "            Conv2d-5        [-1, 128, 150, 150]         147,584\n",
            "         MaxPool2d-6          [-1, 128, 75, 75]               0\n",
            "            Conv2d-7          [-1, 256, 75, 75]         295,168\n",
            "            Conv2d-8          [-1, 256, 75, 75]         590,080\n",
            "            Conv2d-9          [-1, 256, 75, 75]         590,080\n",
            "        MaxPool2d-10          [-1, 256, 38, 38]               0\n",
            "           Conv2d-11          [-1, 512, 38, 38]       1,180,160\n",
            "           Conv2d-12          [-1, 512, 38, 38]       2,359,808\n",
            "           Conv2d-13          [-1, 512, 38, 38]       2,359,808\n",
            "        MaxPool2d-14          [-1, 512, 19, 19]               0\n",
            "           Conv2d-15          [-1, 512, 19, 19]       2,359,808\n",
            "           Conv2d-16          [-1, 512, 19, 19]       2,359,808\n",
            "           Conv2d-17          [-1, 512, 19, 19]       2,359,808\n",
            "        MaxPool2d-18          [-1, 512, 19, 19]               0\n",
            "           Conv2d-19         [-1, 1024, 19, 19]       4,719,616\n",
            "           Conv2d-20         [-1, 1024, 19, 19]       1,049,600\n",
            "           Linear-21                  [-1, 512]     189,268,480\n",
            "           Linear-22                  [-1, 256]         131,328\n",
            "           Linear-23                   [-1, 21]           5,397\n",
            "================================================================\n",
            "Total params: 209,889,109\n",
            "Trainable params: 209,889,109\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.03\n",
            "Forward/backward pass size (MB): 213.71\n",
            "Params size (MB): 800.66\n",
            "Estimated Total Size (MB): 1015.41\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:99: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKYfx6MUhmu-"
      },
      "source": [
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(vgg.parameters(),lr=LR,weight_decay=WEIGHT_DECAY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TmE0Zj-hmu-",
        "outputId": "1a083fca-8826-4563-82da-3521e3b80642"
      },
      "source": [
        "%%time\n",
        "for epoch in range(EPOCH):\n",
        "    vgg.train()\n",
        "    train_loss=[]\n",
        "    for step,(img,labels) in enumerate(train_dl):\n",
        "        labels=torch.tensor(labels)\n",
        "        labels=labels.long()\n",
        "        labels=labels.to(\"cuda:0\")\n",
        "        img=img.to(\"cuda:0\")\n",
        "        pred=vgg(img)\n",
        "        loss=criterion(pred,labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}, train loss: {np.mean(train_loss)}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:99: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, train loss: 3.044392997228757\n",
            "Epoch 2, train loss: 3.044129522288748\n",
            "Epoch 3, train loss: 3.0437842816036613\n",
            "Epoch 4, train loss: 3.04317569732666\n",
            "Epoch 5, train loss: 3.041479242065868\n",
            "Epoch 6, train loss: 3.0227061363797896\n",
            "Epoch 7, train loss: 3.0169280278776083\n",
            "Epoch 8, train loss: 2.993815061006471\n",
            "Epoch 9, train loss: 2.979552818029419\n",
            "Epoch 10, train loss: 2.961246898217861\n",
            "CPU times: user 1h 22min 47s, sys: 9.59 s, total: 1h 22min 57s\n",
            "Wall time: 1h 22min 51s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2z70rkt5yTG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}