{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "scale_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "metadata": {
      "interpreter": {
        "hash": "7a42647e4c83c11f35b3df9f93177de0f6edb84fc1cf48b1d5ae91b569103241"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5f74685579f2415b9641c0133e58c00e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_13b536fd7f3f4b4ba7ab056d41176f65",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_93e4dac2fba0468abc410eaf5e9710c5",
              "IPY_MODEL_6f99f04548744b65b9be209044237aa3"
            ]
          }
        },
        "13b536fd7f3f4b4ba7ab056d41176f65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "93e4dac2fba0468abc410eaf5e9710c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b959290f4c8d4876b42bf2269f2e3cc7",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2c1dd46a0b774518a4c5e2ca25eac328"
          }
        },
        "6f99f04548744b65b9be209044237aa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_913085f64f2044af94f17a41a7c3b4bd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [00:03&lt;00:00, 172MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1aa8d85e3e1a466ab64e3813f084c9da"
          }
        },
        "b959290f4c8d4876b42bf2269f2e3cc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2c1dd46a0b774518a4c5e2ca25eac328": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "913085f64f2044af94f17a41a7c3b4bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1aa8d85e3e1a466ab64e3813f084c9da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8OyDt9vhmuw"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torchvision\n",
        "from PIL import Image"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID5AwyEWkfW2",
        "outputId": "cc02311f-6a4c-4081-e7af-2e320db6029b"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMDsQrOThrEi",
        "outputId": "584624a2-092f-41df-9caf-5484d9c15ba2"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jre4rm2hmuz",
        "outputId": "b5bc28f5-2071-4437-8009-144955c29a4c"
      },
      "source": [
        "PATH=\"/content/gdrive/MyDrive\"\n",
        "os.listdir(PATH)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Colab Notebooks', 'train_images', 'csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxj6x5lehmu0"
      },
      "source": [
        "# Declaring Global Varibles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUpTuKXNhmu0"
      },
      "source": [
        "CSV_PATH=PATH+\"/csv\"\n",
        "IMAGE_PATH=PATH+\"/train_images\"\n",
        "BS=8\n",
        "LR=0.1\n",
        "WEIGHT_DECAY=5E-4\n",
        "EPOCH=10"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYO3KYumhmu1"
      },
      "source": [
        "# Importing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "n5SuB1FLhmu1",
        "outputId": "cd1f59cf-dda9-4b1f-cea6-c8623ede9b78"
      },
      "source": [
        "data=pd.read_csv(CSV_PATH+\"/train.csv\")\n",
        "data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>label</th>\n",
              "      <th>template_name</th>\n",
              "      <th>xmin</th>\n",
              "      <th>ymin</th>\n",
              "      <th>xmax</th>\n",
              "      <th>ymax</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0181 copie.jpg</td>\n",
              "      <td>0181 copie_0</td>\n",
              "      <td>0181 copie_0_0.jpg</td>\n",
              "      <td>216</td>\n",
              "      <td>696</td>\n",
              "      <td>545</td>\n",
              "      <td>1040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0181 copie.jpg</td>\n",
              "      <td>0181 copie_0_flipped</td>\n",
              "      <td>0181 copie_0_flipped_0.jpg</td>\n",
              "      <td>852</td>\n",
              "      <td>687</td>\n",
              "      <td>1210</td>\n",
              "      <td>1038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0181 copie.jpg</td>\n",
              "      <td>0181 copie_1</td>\n",
              "      <td>0181 copie_1_0.jpg</td>\n",
              "      <td>557</td>\n",
              "      <td>701</td>\n",
              "      <td>861</td>\n",
              "      <td>1080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0181 copie.jpg</td>\n",
              "      <td>0181 copie_2</td>\n",
              "      <td>0181 copie_2_0.jpg</td>\n",
              "      <td>194</td>\n",
              "      <td>1201</td>\n",
              "      <td>419</td>\n",
              "      <td>1380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0181 copie.jpg</td>\n",
              "      <td>0181 copie_2</td>\n",
              "      <td>0181 copie_2_1.jpg</td>\n",
              "      <td>1019</td>\n",
              "      <td>1201</td>\n",
              "      <td>1275</td>\n",
              "      <td>1379</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       image_name                 label  ...  xmax  ymax\n",
              "0  0181 copie.jpg          0181 copie_0  ...   545  1040\n",
              "1  0181 copie.jpg  0181 copie_0_flipped  ...  1210  1038\n",
              "2  0181 copie.jpg          0181 copie_1  ...   861  1080\n",
              "3  0181 copie.jpg          0181 copie_2  ...   419  1380\n",
              "4  0181 copie.jpg          0181 copie_2  ...  1275  1379\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-q_yEfRhmu2",
        "outputId": "00975b6f-1bec-4bc1-e4dc-f0732a765e39"
      },
      "source": [
        "image_names=data['image_name'].unique()\n",
        "print(f\"We have {len(image_names)} images\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 18 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbRGIEHmhmu2"
      },
      "source": [
        "# Store the arrays of the image\n",
        "image_names=list(data['image_name'].unique())\n",
        "imageArray=pd.DataFrame({\"image_name\":[],\"image_array\":[]})\n",
        "for im in image_names:\n",
        "  img=Image.open(IMAGE_PATH+\"/\"+im)\n",
        "  imageArray=pd.concat([imageArray,pd.DataFrame({\"image_name\":[im],\"image_array\":[np.array(img)]})])\n",
        "  del img"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Mcp3xykchmu2",
        "outputId": "68991850-e3f6-483d-c0c0-1c04d580dcf4"
      },
      "source": [
        "imageArray.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>image_array</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0181 copie.jpg</td>\n",
              "      <td>[[[145, 138, 132], [140, 133, 127], [140, 133,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0183 copie.jpg</td>\n",
              "      <td>[[[153, 146, 140], [165, 158, 152], [133, 126,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0185 copie.jpg</td>\n",
              "      <td>[[[163, 156, 148], [156, 149, 141], [151, 144,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0187 copie.jpg</td>\n",
              "      <td>[[[151, 144, 138], [151, 144, 138], [148, 141,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0189 copie.jpg</td>\n",
              "      <td>[[[143, 136, 130], [146, 139, 133], [133, 126,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       image_name                                        image_array\n",
              "0  0181 copie.jpg  [[[145, 138, 132], [140, 133, 127], [140, 133,...\n",
              "0  0183 copie.jpg  [[[153, 146, 140], [165, 158, 152], [133, 126,...\n",
              "0  0185 copie.jpg  [[[163, 156, 148], [156, 149, 141], [151, 144,...\n",
              "0  0187 copie.jpg  [[[151, 144, 138], [151, 144, 138], [148, 141,...\n",
              "0  0189 copie.jpg  [[[143, 136, 130], [146, 139, 133], [133, 126,..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm7RxbEohmu3"
      },
      "source": [
        "# Pretext Task\n",
        "\n",
        "    > Predict Scale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63UjbMFshmu3"
      },
      "source": [
        "# We won't save the arrays of the rescaled image, we will save the scaling factor and coordinates for crop\n",
        "scaleData=pd.DataFrame({\"image_name\":[],\"x_min\":[],\"y_min\":[],\"x_max\":[],\"y_max\":[],\"scaling_factor\":[]})\n",
        "scaleData[\"x_min\"]=scaleData[\"image_name\"].apply(lambda x:0)\n",
        "scaleData[\"y_min\"]=scaleData[\"image_name\"].apply(lambda x:0)\n",
        "scaleData[\"x_max\"]=scaleData[\"image_name\"].apply(lambda x:\n",
        "                                                 imageArray[imageArray[\"image_name\"]==x][\"image_array\"][0].shape[1])\n",
        "scaleData[\"y_max\"]=scaleData[\"image_name\"].apply(lambda x:\n",
        "                                                 imageArray[imageArray[\"image_name\"]==x][\"image_array\"][0].shape[0])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0JMLxvEhmu3"
      },
      "source": [
        "# We will randomly crop an image and scale it to 300 * 300 and store the scaling factor as a target variable\n",
        "scaling_factors=np.arange(1,3.1,0.1)\n",
        "def rescale_crop(num,image_name):\n",
        "    \"\"\" Randomly crop an image and scale it to 300 * 300 \"\"\"\n",
        "    global scaleData\n",
        "    global data\n",
        "    global scaling_factors\n",
        "\n",
        "    h,w=imageArray[imageArray['image_name']==image_name][\"image_array\"].values[0].shape[:2]\n",
        "\n",
        "    # Choose randomly a scaling factor\n",
        "    sc=np.random.choice(scaling_factors)\n",
        "\n",
        "    # Randomly choose a height for image between 400 to 1000\n",
        "    new_w=int(300*sc)\n",
        "    new_h=int(300*sc)\n",
        "\n",
        "    # Choose randomly the coordinates\n",
        "    new_x=int(np.random.randint(0,w-2*new_w))\n",
        "    new_y=int(np.random.randint(0,h-new_h))\n",
        "\n",
        "    # image name\n",
        "    name=image_name.split(\".\")[0]+\"_scale_\"+str(num)+\".jpg\"\n",
        "\n",
        "    scaleData=pd.concat([scaleData,pd.DataFrame({\"image_name\":[name],\"x_min\":[new_x],\n",
        "    \"y_min\":[new_y],\"x_max\":[new_x+new_w],\"y_max\":[new_y+new_h],\"scaling_factor\":[sc]})])\n",
        "     \n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RG39_yMhmu4",
        "outputId": "801ff157-64ce-4960-c045-b07b20a0b1ce"
      },
      "source": [
        "%%time\n",
        "for im in image_names:\n",
        "    for i in range(200):\n",
        "        rescale_crop(i,im)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 7.52 s, sys: 143 ms, total: 7.66 s\n",
            "Wall time: 7.49 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78wUzLS-hmu4"
      },
      "source": [
        "le=LabelEncoder()\n",
        "scaleData['scaling_factor']=le.fit_transform(scaleData['scaling_factor'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "KxAJHUFqhmu4",
        "outputId": "0f8dbf25-331d-411f-b252-a18774ae64a5"
      },
      "source": [
        "scaleData.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>x_min</th>\n",
              "      <th>y_min</th>\n",
              "      <th>x_max</th>\n",
              "      <th>y_max</th>\n",
              "      <th>scaling_factor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0181 copie_scale_0.jpg</td>\n",
              "      <td>1176.0</td>\n",
              "      <td>1794.0</td>\n",
              "      <td>1596.0</td>\n",
              "      <td>2214.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0181 copie_scale_1.jpg</td>\n",
              "      <td>125.0</td>\n",
              "      <td>2666.0</td>\n",
              "      <td>785.0</td>\n",
              "      <td>3326.0</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0181 copie_scale_2.jpg</td>\n",
              "      <td>752.0</td>\n",
              "      <td>3110.0</td>\n",
              "      <td>1202.0</td>\n",
              "      <td>3560.0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0181 copie_scale_3.jpg</td>\n",
              "      <td>661.0</td>\n",
              "      <td>3131.0</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>3731.0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0181 copie_scale_4.jpg</td>\n",
              "      <td>150.0</td>\n",
              "      <td>2990.0</td>\n",
              "      <td>780.0</td>\n",
              "      <td>3620.0</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               image_name   x_min   y_min   x_max   y_max  scaling_factor\n",
              "0  0181 copie_scale_0.jpg  1176.0  1794.0  1596.0  2214.0               4\n",
              "0  0181 copie_scale_1.jpg   125.0  2666.0   785.0  3326.0              12\n",
              "0  0181 copie_scale_2.jpg   752.0  3110.0  1202.0  3560.0               5\n",
              "0  0181 copie_scale_3.jpg   661.0  3131.0  1261.0  3731.0              10\n",
              "0  0181 copie_scale_4.jpg   150.0  2990.0   780.0  3620.0              11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmpSq8oshmu5"
      },
      "source": [
        "\n",
        "#scaleData.to_csv(CSV_PATH+\"/\"+\"scaleData.csv\",index=None)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGPA7coOhmu7"
      },
      "source": [
        "class ScaleDataset(Dataset):\n",
        "    def __init__(self, dataset, image_dataset, is_test=False, transform=None):\n",
        "        #self.annotation_folder_path = csv_path\n",
        "        self.dataset=dataset\n",
        "        self.image_dataset=image_dataset\n",
        "        self.all_images=self.dataset['image_name'].unique()\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        img_name=self.all_images[idx]\n",
        "        original_img_name=img_name.split(\"_\")[0]+\".jpg\"\n",
        "        coord=self.dataset[self.dataset['image_name']==img_name][[\"x_min\",\"y_min\",\"x_max\",\"y_max\"]].values[0]\n",
        "        img=Image.fromarray(self.image_dataset[self.image_dataset['image_name']==original_img_name]['image_array'].values[0][\n",
        "            int(coord[1]):int(coord[3]),int(coord[0]):int(coord[2])])\n",
        "        #img=img.convert(\"RGB\")\n",
        "\n",
        "        if not self.is_test:\n",
        "            annotations=self.dataset[self.dataset['image_name']==img_name]\n",
        "\n",
        "            #self.box = self.get_xy(annotations)\n",
        "\n",
        "            #self.new_box = torch.FloatTensor(self.box_resize(self.box, img))\n",
        "            if self.transform is not None:\n",
        "                img = self.transform(img)\n",
        "            \n",
        "\n",
        "            self.labels=torch.FloatTensor(annotations['scaling_factor'].values)\n",
        "\n",
        "            return img, self.labels\n",
        "        else:\n",
        "            return img\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.all_images)\n",
        "        \n",
        "    \n",
        "    def collate_fn(self, batch):\n",
        "        \"\"\"\n",
        "        :param batch: an iterable of N sets from __getitem__()\n",
        "        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
        "        \"\"\"\n",
        "\n",
        "        images = list()\n",
        "        labels = list()\n",
        "#         difficulties = list()\n",
        "\n",
        "        for b in batch:\n",
        "            images.append(b[0])\n",
        "            labels.append(b[1])\n",
        "#             difficulties.append(b[3])\n",
        "\n",
        "        images = torch.stack(images, dim=0)\n",
        "\n",
        "        return images, labels\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6aLY8K0hmu7"
      },
      "source": [
        "tsfm = transforms.Compose([\n",
        "    transforms.Resize([300,300]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0Tkpn2qhmu7"
      },
      "source": [
        "x_train,x_test=train_test_split(scaleData,test_size=0.15,random_state=1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EML7sjdPhmu8"
      },
      "source": [
        "train_ds = ScaleDataset(x_train,imageArray,transform=tsfm)\n",
        "train_dl = DataLoader(train_ds, batch_size=BS, shuffle=True, collate_fn=train_ds.collate_fn)\n",
        "\n",
        "valid_ds = ScaleDataset(x_test,imageArray, transform=tsfm)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=BS, shuffle=True, collate_fn=valid_ds.collate_fn)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V__XS1NDhmu8"
      },
      "source": [
        "def decimate(tensor, m):\n",
        "    \"\"\"\n",
        "    Decimate a tensor by a factor 'm', i.e. downsample by keeping every 'm'th value.\n",
        "    This is used when we convert FC layers to equivalent Convolutional layers, BUT of a smaller size.\n",
        "    :param tensor: tensor to be decimated\n",
        "    :param m: list of decimation factors for each dimension of the tensor; None if not to be decimated along a dimension\n",
        "    :return: decimated tensor\n",
        "    \"\"\"\n",
        "    assert tensor.dim() == len(m)\n",
        "    for d in range(tensor.dim()):\n",
        "        if m[d] is not None:\n",
        "            tensor = tensor.index_select(dim=d,\n",
        "                                         index=torch.arange(start=0, end=tensor.size(d), step=m[d]).long())\n",
        "\n",
        "    return tensor\n",
        "class VGGBase(nn.Module):\n",
        "    \"\"\"\n",
        "    VGG base convolutions to produce lower-level feature maps.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(VGGBase, self).__init__()\n",
        "\n",
        "        # Standard convolutional layers in VGG16\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)  # stride = 1, by default\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)  # ceiling (not floor) here for even dims\n",
        "\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # retains size because stride is 1 (and padding)\n",
        "\n",
        "        # Replacements for FC6 and FC7 in VGG16\n",
        "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)  # atrous convolution\n",
        "\n",
        "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "        # Load pretrained layers\n",
        "        self.load_pretrained_layers()\n",
        "\n",
        "        # Linear Layers\n",
        "        self.linear1=nn.Linear(in_features=1024*19*19,out_features=512)\n",
        "        self.linear2=nn.Linear(in_features=512,out_features=256)\n",
        "        self.output=nn.Linear(in_features=256,out_features=21)\n",
        "\n",
        "\n",
        "    def forward(self, image):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n",
        "        :return: lower-level feature maps conv4_3 and conv7\n",
        "        \"\"\"\n",
        "        out = F.relu(self.conv1_1(image))  # (N, 64, 300, 300)\n",
        "        out = F.relu(self.conv1_2(out))  # (N, 64, 300, 300)\n",
        "        out = self.pool1(out)  # (N, 64, 150, 150)\n",
        "\n",
        "        out = F.relu(self.conv2_1(out))  # (N, 128, 150, 150)\n",
        "        out = F.relu(self.conv2_2(out))  # (N, 128, 150, 150)\n",
        "        out = self.pool2(out)  # (N, 128, 75, 75)\n",
        "\n",
        "        out = F.relu(self.conv3_1(out))  # (N, 256, 75, 75)\n",
        "        out = F.relu(self.conv3_2(out))  # (N, 256, 75, 75)\n",
        "        out = F.relu(self.conv3_3(out))  # (N, 256, 75, 75)\n",
        "        out = self.pool3(out)  # (N, 256, 38, 38), it would have been 37 if not for ceil_mode = True\n",
        "\n",
        "        out = F.relu(self.conv4_1(out))  # (N, 512, 38, 38)\n",
        "        out = F.relu(self.conv4_2(out))  # (N, 512, 38, 38)\n",
        "        out = F.relu(self.conv4_3(out))  # (N, 512, 38, 38)\n",
        "        conv4_3_feats = out  # (N, 512, 38, 38)\n",
        "        out = self.pool4(out)  # (N, 512, 19, 19)\n",
        "\n",
        "        out = F.relu(self.conv5_1(out))  # (N, 512, 19, 19)\n",
        "        out = F.relu(self.conv5_2(out))  # (N, 512, 19, 19)\n",
        "        out = F.relu(self.conv5_3(out))  # (N, 512, 19, 19)\n",
        "        out = self.pool5(out)  # (N, 512, 19, 19), pool5 does not reduce dimensions\n",
        "\n",
        "        out = F.relu(self.conv6(out))  # (N, 1024, 19, 19)\n",
        "\n",
        "        conv7_feats = F.relu(self.conv7(out))  # (N, 1024, 19, 19)\n",
        "\n",
        "        # Linear Layers\n",
        "        flattened_array=conv7_feats.reshape(-1,1024*19*19)\n",
        "        out=F.relu(self.linear1(flattened_array))\n",
        "        out=F.relu(self.linear2(out))\n",
        "        output=F.softmax(self.output(out))\n",
        "\n",
        "        # Lower-level feature maps\n",
        "        return output\n",
        "\n",
        "    def load_pretrained_layers(self):\n",
        "        \"\"\"\n",
        "        As in the paper, we use a VGG-16 pretrained on the ImageNet task as the base network.\n",
        "        There's one available in PyTorch, see https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.vgg16\n",
        "        We copy these parameters into our network. It's straightforward for conv1 to conv5.\n",
        "        However, the original VGG-16 does not contain the conv6 and con7 layers.\n",
        "        Therefore, we convert fc6 and fc7 into convolutional layers, and subsample by decimation. See 'decimate' in utils.py.\n",
        "        \"\"\"\n",
        "        # Current state of base\n",
        "        state_dict = self.state_dict()\n",
        "        param_names = list(state_dict.keys())\n",
        "\n",
        "        # Pretrained VGG base\n",
        "        pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()\n",
        "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
        "\n",
        "        # Transfer conv. parameters from pretrained model to current model\n",
        "        for i, param in enumerate(param_names[:-4]):  # excluding conv6 and conv7 parameters\n",
        "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
        "\n",
        "        # Convert fc6, fc7 to convolutional layers, and subsample (by decimation) to sizes of conv6 and conv7\n",
        "        # fc6\n",
        "        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)  # (4096, 512, 7, 7)\n",
        "        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']  # (4096)\n",
        "        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n",
        "        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])  # (1024)\n",
        "        # fc7\n",
        "        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)  # (4096, 4096, 1, 1)\n",
        "        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']  # (4096)\n",
        "        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None])  # (1024, 1024, 1, 1)\n",
        "        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])  # (1024)\n",
        "\n",
        "        # Note: an FC layer of size (K) operating on a flattened version (C*H*W) of a 2D image of size (C, H, W)...\n",
        "        # ...is equivalent to a convolutional layer with kernel size (H, W), input channels C, output channels K...\n",
        "        # ...operating on the 2D image of size (C, H, W) without padding\n",
        "\n",
        "        self.load_state_dict(state_dict)\n",
        "\n",
        "        print(\"\\nLoaded base model.\\n\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154,
          "referenced_widgets": [
            "5f74685579f2415b9641c0133e58c00e",
            "13b536fd7f3f4b4ba7ab056d41176f65",
            "93e4dac2fba0468abc410eaf5e9710c5",
            "6f99f04548744b65b9be209044237aa3",
            "b959290f4c8d4876b42bf2269f2e3cc7",
            "2c1dd46a0b774518a4c5e2ca25eac328",
            "913085f64f2044af94f17a41a7c3b4bd",
            "1aa8d85e3e1a466ab64e3813f084c9da"
          ]
        },
        "id": "AIIpGNPXhmu9",
        "outputId": "307382d4-3ffd-492d-e775-067b57db331b"
      },
      "source": [
        "vgg=VGGBase().to(\"cuda:0\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f74685579f2415b9641c0133e58c00e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Loaded base model.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVX-giE2hmu9",
        "outputId": "38d2b6c7-d818-40f8-e365-b7b32147954a"
      },
      "source": [
        "summary(vgg,(3,300,300))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 300, 300]           1,792\n",
            "            Conv2d-2         [-1, 64, 300, 300]          36,928\n",
            "         MaxPool2d-3         [-1, 64, 150, 150]               0\n",
            "            Conv2d-4        [-1, 128, 150, 150]          73,856\n",
            "            Conv2d-5        [-1, 128, 150, 150]         147,584\n",
            "         MaxPool2d-6          [-1, 128, 75, 75]               0\n",
            "            Conv2d-7          [-1, 256, 75, 75]         295,168\n",
            "            Conv2d-8          [-1, 256, 75, 75]         590,080\n",
            "            Conv2d-9          [-1, 256, 75, 75]         590,080\n",
            "        MaxPool2d-10          [-1, 256, 38, 38]               0\n",
            "           Conv2d-11          [-1, 512, 38, 38]       1,180,160\n",
            "           Conv2d-12          [-1, 512, 38, 38]       2,359,808\n",
            "           Conv2d-13          [-1, 512, 38, 38]       2,359,808\n",
            "        MaxPool2d-14          [-1, 512, 19, 19]               0\n",
            "           Conv2d-15          [-1, 512, 19, 19]       2,359,808\n",
            "           Conv2d-16          [-1, 512, 19, 19]       2,359,808\n",
            "           Conv2d-17          [-1, 512, 19, 19]       2,359,808\n",
            "        MaxPool2d-18          [-1, 512, 19, 19]               0\n",
            "           Conv2d-19         [-1, 1024, 19, 19]       4,719,616\n",
            "           Conv2d-20         [-1, 1024, 19, 19]       1,049,600\n",
            "           Linear-21                  [-1, 512]     189,268,480\n",
            "           Linear-22                  [-1, 256]         131,328\n",
            "           Linear-23                   [-1, 21]           5,397\n",
            "================================================================\n",
            "Total params: 209,889,109\n",
            "Trainable params: 209,889,109\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.03\n",
            "Forward/backward pass size (MB): 213.71\n",
            "Params size (MB): 800.66\n",
            "Estimated Total Size (MB): 1015.41\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:99: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKYfx6MUhmu-"
      },
      "source": [
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.SGD(vgg.parameters(),lr=LR,weight_decay=WEIGHT_DECAY)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TmE0Zj-hmu-",
        "outputId": "bc948d0f-48de-4ef5-8d27-625ab58136cf"
      },
      "source": [
        "\n",
        "for epoch in range(EPOCH):\n",
        "    vgg.train()\n",
        "    train_loss=[]\n",
        "    test_loss=[]\n",
        "    for step,(img,labels) in enumerate(train_dl):\n",
        "        labels=torch.tensor(labels)\n",
        "        labels=labels.long()\n",
        "        labels=labels.to(\"cuda:0\")\n",
        "        img=img.to(\"cuda:0\")\n",
        "        pred=vgg(img)\n",
        "        loss=criterion(pred,labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for step,(img,labels) in enumerate(valid_dl):\n",
        "        labels=torch.tensor(labels)\n",
        "        labels=labels.long()\n",
        "        labels=labels.to(\"cuda:0\")\n",
        "        img=img.to(\"cuda:0\")\n",
        "        pred=vgg(img)\n",
        "        val_loss=criterion(pred,labels)\n",
        "\n",
        "        test_loss.append(val_loss.item())\n",
        "        \n",
        "    \n",
        "    print(f\"Epoch {epoch+1}, train loss: {np.mean(train_loss)}, test loss: {np.mean(test_loss)}\")\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:99: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, train loss: 3.075848208395059, test loss: 3.0845688651589787\n",
            "Epoch 2, train loss: 3.0758481623298506, test loss: 3.084569313946892\n",
            "Epoch 3, train loss: 3.0758481965675055, test loss: 3.0845704429289875\n",
            "Epoch 4, train loss: 3.0751961603488374, test loss: 3.084570463965921\n",
            "Epoch 5, train loss: 3.0758493594027687, test loss: 3.084570982876946\n",
            "Epoch 6, train loss: 3.075850527218054, test loss: 3.0845701974980972\n",
            "Epoch 7, train loss: 3.074603326015622, test loss: 3.044219556976767\n",
            "Epoch 8, train loss: 3.0719041836790875, test loss: 3.084566957810346\n",
            "Epoch 9, train loss: 3.0758461404406994, test loss: 3.082728726022384\n",
            "Epoch 10, train loss: 3.0758460638728526, test loss: 3.0845668105518116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2z70rkt5yTG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}