{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tfo-t7pS-Lx",
        "outputId": "60a25386-6e8a-46f6-fe74-f5fa4df37d97"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVG1BNZmTDUu",
        "outputId": "c6c0e1ab-d5ec-456c-de78-9a98e6603854"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/MyDrive\")\n",
        "os.listdir()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Colab Notebooks', 'train_images', 'csv', 'SSD']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNdCqf8gTI5c"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np # Linear Algebra\n",
        "import gc\n",
        "import pandas as pd # Data Processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import torch\n",
        "from torchvision import models\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision.ops import RoIAlign\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import timeit\n",
        "from SSD.box_utils import cxcy_to_gcxgcy,cxcy_to_xy,gcxgcy_to_cxcy,xy_to_cxcy,find_jaccard_overlap,get_target_image\n",
        "from SSD.ssd import SSD\n",
        "from SSD.loss import MultiBoxLoss,DecoderLoss\n",
        "from SSD.dataset import SSDDataset\n",
        "from SSD.iou import bb_intersection_over_union\n",
        "from SSD.decoder import Decoder"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTEpw6yjTK-D"
      },
      "source": [
        "imageData=pd.read_csv(\"/content/gdrive/MyDrive/csv/imageData_3.csv\")\n",
        "data=pd.read_csv(\"/content/gdrive/MyDrive/csv/data_3.csv\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl2VZfUzTd37"
      },
      "source": [
        "image_names=list(data['image_name'].unique())\n",
        "imageArray=pd.DataFrame({\"image_name\":[],\"image_array\":[]})\n",
        "for im in image_names:\n",
        "  try:\n",
        "    img=Image.open(\"train_images/\"+im)\n",
        "    imageArray=pd.concat([imageArray,pd.DataFrame({\"image_name\":[im],\"image_array\":[np.array(img)]})])\n",
        "    del img\n",
        "  except:\n",
        "    pass"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnoGB-ccTuKx"
      },
      "source": [
        "# Dataset Genertation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mk3oxLUTO8q"
      },
      "source": [
        "class DecoderDataset(Dataset):\n",
        "    def __init__(self, data, imageData, imageArray, is_test=False, transform=None):\n",
        "        #self.annotation_folder_path = csv_path\n",
        "        self.data=data # Contains the information about bounding boxes\n",
        "        self.imageData=imageData # Contains the coordinate of the cropped images\n",
        "        self.imageArray=imageArray # Contains the arrays of the original 18 images\n",
        "        self.all_images=self.data['image_name'].unique()\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.all_images[idx]\n",
        "        if \"_\" in img_name:\n",
        "          original_img_name=img_name.split(\"_\")[0]+\".jpg\"\n",
        "        else:\n",
        "          original_img_name=img_name\n",
        "        coord=self.imageData[self.imageData['image_name']==img_name][[\"x_min\",\"y_min\",\"x_max\",\"y_max\"]].values[0]\n",
        "        img = Image.fromarray(self.imageArray[self.imageArray['image_name']==original_img_name]['image_array'].values[0][\n",
        "            int(coord[1]):int(coord[3]),int(coord[0]):int(coord[2]),:])\n",
        "        img = img.convert('RGB')\n",
        "        \n",
        "        if not self.is_test:\n",
        "            annotations=self.data[self.data['image_name']==img_name]\n",
        "\n",
        "            self.box = self.get_xy(annotations)\n",
        "\n",
        "            self.new_box = torch.cuda.FloatTensor(self.box_resize(self.box, img))\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            \n",
        "\n",
        "            #self.labels=torch.FloatTensor(annotations['label'].values).cuda()\n",
        "\n",
        "            \"\"\"# Encode the labels with Int\n",
        "            self.le=LabelEncoder()\n",
        "            self.labels=torch.FloatTensor(self.le.fit_transform(self.labels))\"\"\"\n",
        "\n",
        "            return img_name,img.cuda(), self.new_box\n",
        "        else:\n",
        "            return img_name,img.cuda()\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.all_images)\n",
        "        \n",
        "    def get_xy(self, annotation):\n",
        "        boxes=torch.FloatTensor(annotation[['xmin','ymin','xmax','ymax']].values)\n",
        "        return boxes\n",
        "        \n",
        "    def box_resize(self, box, img, dims=(300, 300)):\n",
        "        old_dims = torch.cuda.FloatTensor([img.width, img.height, img.width, img.height]).unsqueeze(0)\n",
        "        new_box = box.cuda() / old_dims\n",
        "        new_dims = torch.cuda.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\n",
        "        #new_box = new_box * new_dims\n",
        "        \n",
        "        return new_box\n",
        "    \n",
        "    def collate_fn(self, batch):\n",
        "        \"\"\"\n",
        "        :param batch: an iterable of N sets from __getitem__()\n",
        "        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
        "        \"\"\"\n",
        "        original_images=list() # Name of the images\n",
        "        images = list()\n",
        "        boxes = list()\n",
        "        labels = list()\n",
        "#         difficulties = list()\n",
        "\n",
        "        for b in batch:\n",
        "            original_images.append(b[0])\n",
        "            images.append(b[1])\n",
        "            boxes.append(b[2])\n",
        "            #labels.append(b[3])\n",
        "#             difficulties.append(b[3])\n",
        "\n",
        "        images = torch.stack(images, dim=0)\n",
        "\n",
        "        return original_images,images, boxes  # tensor (N, 3, 300, 300),"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPBAo5KDTSSP"
      },
      "source": [
        "tsfm = transforms.Compose([\n",
        "    transforms.Resize([300,300]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "tsfmV2=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "# Batch Size\n",
        "BS=8"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVz1cNMKTUcj"
      },
      "source": [
        "train_ds = DecoderDataset(data.iloc[4606:,:], imageData,imageArray, transform=tsfm)\n",
        "train_dl = DataLoader(train_ds, batch_size=BS, shuffle=True, collate_fn=train_ds.collate_fn)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg9u32iAsS9o"
      },
      "source": [
        "x=next(iter(train_dl))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YguDB1WsTqHo"
      },
      "source": [
        "# Load Pretrained SSD Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRQA2az8TWaA"
      },
      "source": [
        "N_CLASSES=data['label'].nunique()+1\n",
        "model=model = SSD(n_classes=N_CLASSES)\n",
        "model.load_state_dict(torch.load(\"/content/gdrive/MyDrive/SSD/model_giou.pth\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHUl1V0QXxLY"
      },
      "source": [
        "class DetectorDecoder(nn.Module):\n",
        "  def __init__(self,detector=None,output_size=(5,5),num_filter=50000):\n",
        "    super(DetectorDecoder,self).__init__()\n",
        "    \"\"\" \n",
        "    Parameter:\n",
        "    detector --> Object Detector\n",
        "    output_size --> Output size of ROI Align step. Default (5,5)\n",
        "    filter --> Number of feature maps after merging all the feature maps\n",
        "    \"\"\"\n",
        "\n",
        "    # The feature maps in detector will work as encoded information.\n",
        "    self.detector=detector\n",
        "    self.output_size=output_size\n",
        "    self.num_filter=num_filter\n",
        "        \n",
        "     # ROIALIGN\n",
        "    self.roialign=RoIAlign(self.output_size,1,-1)\n",
        "    \n",
        "\n",
        "    # Decoding Layers\n",
        "    self.convT_1=nn.ConvTranspose2d(self.num_filter,1024,kernel_size=3,stride=2) #(256,39,39)\n",
        "    self.convT_2=nn.ConvTranspose2d(1024,512,kernel_size=3,stride=2) #(128,79,79)\n",
        "    self.upsample_1=nn.Upsample((80,80)) #(128,80,80)\n",
        "\n",
        "    self.convT_3=nn.ConvTranspose2d(512,128,kernel_size=3,stride=2)\n",
        "    self.upsample_2=nn.Upsample((160,160))\n",
        "\n",
        "    self.convT_4=nn.ConvTranspose2d(128,32,kernel_size=3,stride=2)\n",
        "    self.convT_5=nn.ConvTranspose2d(32,3,kernel_size=1,stride=2)\n",
        "    self.upsample_3=nn.Upsample((640,640))\n",
        "\n",
        "  def forward(self,mask):\n",
        "    x,self.boxes=mask\n",
        "\n",
        "    # Get RoI's offset\n",
        "    self.locs,_=self.detector(x)\n",
        "\n",
        "\n",
        "    # Get all the feature maps\n",
        "    x=self.get_feature_maps(x)\n",
        "\n",
        "    out=self.convT_1(x)\n",
        "    out=self.convT_2(out)\n",
        "    out=self.upsample_1(out)\n",
        "    out=self.convT_3(out)\n",
        "    out=self.upsample_2(out)\n",
        "    out=self.convT_4(out)\n",
        "    out=self.convT_5(out)\n",
        "    out=self.upsample_3(out)\n",
        "    return out\n",
        "  \n",
        "  def get_feature_maps(self,x):\n",
        "    \"\"\" Get all the feature maps from the ssd detector\"\"\"\n",
        "    self.rescale_factors = nn.Parameter(torch.cuda.FloatTensor(1, 512, 1, 1))  # there are 512 channels in conv4_3_feats\n",
        "    \n",
        "\n",
        "    # Get Conv4 and Conv7 from VGG\n",
        "    self.conv4_3,self.conv7=self.detector.base(x)\n",
        "\n",
        "    # Rescale conv4_3 after L2 norm\n",
        "    norm = self.conv4_3.pow(2).sum(dim=1, keepdim=True).sqrt()  # (N, 1, 38, 38)\n",
        "    self.conv4_3 = self.conv4_3 / norm  # (N, 512, 38, 38)\n",
        "    self.conv4_3 = self.conv4_3 * self.rescale_factors \n",
        "\n",
        "    # Get rest of the feature maps from\n",
        "    self.conv8_2, self.conv9_2, self.conv10_2, self.conv11_2 = self.detector.aux_convs(self.conv7)\n",
        "\n",
        "    # Merge all the Feature maps after roi aligning\n",
        "    feature_map=self.merge_feature_maps()\n",
        "    return feature_map\n",
        "    \n",
        "  \n",
        "  def merge_feature_maps(self):\n",
        "    \"\"\" Merge all Fearure Maps\"\"\"\n",
        "    # ROI Postions in Locs data Dictinary\n",
        "    self.__roi_pos_dict={\"conv4_3\":[0,5776],\"conv7\":[5776,7942],\n",
        "                 \"conv8_2\":[7942,8542],\"conv9_2\":[8542,8692],\"conv10_2\":[8692,8728],\n",
        "                 \"conv11_2\":[8728,8732]}\n",
        "\n",
        "    \"\"\"# Region of Interest Dictionary\n",
        "    self.__roi_dict={\"conv4_3\":self.locs[:,:5776,:],\"conv7\":self.locs[:,5776:7942,:],\"conv8_2\":self.locs[:,7942:8542,:],\n",
        "             \"conv9_2\":self.locs[:,8542:8692,:],\"conv10_2\":self.locs[:,8692:8728,:],\"conv11_2\":self.locs[:,8728:8732,:]}\"\"\"\n",
        "\n",
        "    # Feature Maps Dictionary\n",
        "    self.__feat_map_dict={\"conv4_3\":self.conv4_3,\"conv7\":self.conv7,\n",
        "                 \"conv8_2\":self.conv8_2,\"conv9_2\":self.conv9_2,\"conv10_2\":self.conv10_2,\n",
        "                 \"conv11_2\":self.conv11_2}\n",
        "    \n",
        "    # Find Roi's for conv4_3, since we will use roialign here\n",
        "    self.__roi_conv4_3=self.roi_align(\"conv4_3\",(38,38))\n",
        "    self.__roi_conv7=self.roi_align(\"conv7\",(19,19))\n",
        "    self.__roi_conv8_2=self.roi_align(\"conv8_2\",(10,10))\n",
        "    self.__roi_conv9_2=self.roi_align(\"conv9_2\",(5,5))\n",
        "    self.__roi_conv10_2=self.roi_align(\"conv10_2\",(3,3))\n",
        "    #self.__roi_conv11_2=self.roi_align(\"conv11_2\",(3,3))\n",
        "\n",
        "    feature_map=torch.cat((self.__roi_conv4_3,self.__roi_conv7,self.__roi_conv8_2,self.__roi_conv9_2,self.__roi_conv10_2),dim=1)\n",
        "\n",
        "    # Since Feature maps dimensions are different for every image, we will take first 50000 filters\n",
        "    if feature_map.size(1)>=self.num_filter:\n",
        "      feature_map=feature_map[:,:self.num_filter,:,:]\n",
        "    else:\n",
        "      # We need to upsample\n",
        "      upsample=nn.Upsample(5,self.num_filter)\n",
        "      feature_map=torch.moveaxis(upsample(torch.moveaxis(feature_map,1,-1)),-1,1)\n",
        "    assert feature_map.shape==torch.Size([self.__batch_size,self.num_filter,self.output_size[0],self.output_size[1]])\n",
        "\n",
        "    \n",
        "    return feature_map\n",
        "  \n",
        "  def add_batch_number(self,boxes,batch):\n",
        "    \"\"\" Add Batch Number in the first column of the rois \"\"\"\n",
        "    box_size=boxes.size(0)\n",
        "    return torch.stack((torch.ones((box_size,1,4),device=\"cuda\")*batch,boxes.reshape(box_size,1,-1)),dim=2).reshape(-1,8)[:,3:]\n",
        "  \n",
        "  \n",
        "  def find_roi(self,feature_name,feature_size):\n",
        "    \"\"\" It finds the RoI's for a specific feature name\"\"\"\n",
        "    self.__batch_size=self.locs.size(0)\n",
        "    roi_sp=[]\n",
        "    min_,max_=self.__roi_pos_dict[feature_name]\n",
        "    \n",
        "\n",
        "    for i in range(self.__batch_size):\n",
        "      n_objects=self.boxes[i].size(0)\n",
        "      overlap=find_jaccard_overlap(self.boxes[i],torch.clamp(self.detector.priors_xy[min_:max_,:],min=0))\n",
        "      locs_xy=cxcy_to_xy(gcxgcy_to_cxcy(self.locs[i],self.detector.priors_cxcy))\n",
        "\n",
        "      _, prior_for_each_object = overlap.max(dim=1)\n",
        "      locs_for_map=torch.clamp(locs_xy[min_:max_,:][prior_for_each_object],min=0,max=1)*(feature_size[0]-1)\n",
        "      locs_for_map=self.add_batch_number(locs_for_map,i)\n",
        "      roi_sp.append(locs_for_map)\n",
        "    roi_sp=torch.cat(roi_sp,dim=0)\n",
        "    return roi_sp\n",
        "\n",
        "    \n",
        "  def roi_align(self,feature_name,feature_size):\n",
        "    \"\"\" Roi Alignment \"\"\"\n",
        "    w,h=self.output_size\n",
        "    \n",
        "    self.__roi=self.find_roi(feature_name,feature_size)\n",
        "    aligned_map=self.roialign(self.__feat_map_dict[feature_name],self.__roi)\n",
        "    return aligned_map.reshape(self.__batch_size,-1,w,h)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyHCrpacR042"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L45T9lXHRvdp"
      },
      "source": [
        "EPOCH=10\n",
        "LR=1e-4"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gth4PdUCX1md"
      },
      "source": [
        "decoder=Decoder(model).cuda()\n",
        "criterion=DecoderLoss(\"mse\")\n",
        "optimizer=torch.optim.Adam(decoder.parameters(),lr=LR)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "-s9NGYqRVC6K",
        "outputId": "48fadd7b-6664-46b5-cdc6-8df4164df3a7"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "for epoch in range(1, EPOCH+1):\n",
        "    decoder.train()\n",
        "    train_loss = []\n",
        "    time_start=time.time()\n",
        "    for step, (target_image_names,resized_image,boxes) in enumerate(train_dl):\n",
        "        time_1 = time.time()\n",
        "        target_image=get_target_image(target_image_names)\n",
        "        \n",
        "        reconstructed_image = decoder(resized_image,boxes)\n",
        "        \n",
        "        loss = criterion(reconstructed_image,target_image)\n",
        " \n",
        "        \n",
        "        # Backward prop.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Losses.update(loss.item(), images.size(0))\n",
        "        train_loss.append(loss.item())\n",
        "    time_end=time.time()\n",
        "    total_time=str((time_end-time_start)//60)+\" minutes and \"+ str((time_end-time_start)%60)+\" seconds \"\n",
        "        \n",
        "        \n",
        "    print(\"Time:\",total_time, ' epoch: ', epoch, '/', EPOCH,\n",
        "            'train loss:', '{:.4f}'.format(np.mean(train_loss)))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-f5840bbcf334>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m#target_image=get_target_image(target_image_names)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mreconstructed_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/SSD/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, boxes)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_maps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvT_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvT_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvT_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    840\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    841\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.72 GiB (GPU 0; 14.76 GiB total capacity; 11.39 GiB already allocated; 1.39 GiB free; 12.33 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt0BXbMePm1l",
        "outputId": "49a2aa9d-4fa1-4d17-fb74-74293f528d51"
      },
      "source": [
        "input.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 3, 640, 640])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOFvw6cHfA9Z"
      },
      "source": [
        "x=torch.rand((8,5777,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPMfzVaiig1q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "900ae762-c0d6-4ea7-d55a-8608f17a3378"
      },
      "source": [
        "feature_map_pos={\"conv4_3\":[0,5776],\"conv7\":[5776,7942],\n",
        "                 \"conv8_2\":[7942,8542],\"conv9_2\":[8542,8692],\"conv10_2\":[8692,8728],\n",
        "                 \"conv11_2\":[8728,8732]}\n",
        "\n",
        "feature_map={\"conv4_3\":locs[:,:5776,:],\"conv7\":locs[:,5776:7942,:],\"conv8_2\":locs[:,7942:8542,:],\n",
        "             \"conv9_2\":locs[:,8542:8692,:],\"conv10_2\":locs[:,8692:8728,:],\"conv11_2\":locs[:,8728:8732,:]}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-dbf91a264738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                  \"conv11_2\":[8728,8732]}\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m feature_map={\"conv4_3\":locs[:,:5776,:],\"conv7\":locs[:,5776:7942,:],\"conv8_2\":locs[:,7942:8542,:],\n\u001b[0m\u001b[1;32m      6\u001b[0m              \"conv9_2\":locs[:,8542:8692,:],\"conv10_2\":locs[:,8692:8728,:],\"conv11_2\":locs[:,8728:8732,:]}\n",
            "\u001b[0;31mNameError\u001b[0m: name 'locs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAKdsIcbuT1b",
        "outputId": "d55d451d-77cb-4521-9127-f316f548ecb2"
      },
      "source": [
        "for key in feature_map_pos:\n",
        "  print(key)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv4_3\n",
            "conv7\n",
            "conv8_2\n",
            "conv9_2\n",
            "conv10_2\n",
            "conv11_2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsYLbWE8oS_T"
      },
      "source": [
        "def find_map(box):\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HfzNVKUWMMK"
      },
      "source": [
        "x=next(iter(train_dl))\n",
        "locs,_=model(x[0].cuda())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFXQYsPwW0Ex"
      },
      "source": [
        "conv4_3=model.base(x[0].cuda())[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdbW5ZIOW-3f",
        "outputId": "316cb42b-9d1c-4a11-f629-a20653cbf7a5"
      },
      "source": [
        "conv4_3.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 512, 38, 38])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wWafXu2WYhV"
      },
      "source": [
        "# Get the Real Boxes\n",
        "boxes=x[1][1]\n",
        "\n",
        "# Get the predicted Boxes\n",
        "priors_xy=cxcy_to_xy(model.priors_cxcy)\n",
        "\n",
        "# Get the jaccard overlap\n",
        "overlap=find_jaccard_overlap(boxes,torch.clamp(priors_xy,min=0)[:5776])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAkNaKJCT9u_"
      },
      "source": [
        "temp=overlap.max(dim=1)[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBgqNA8s83F5",
        "outputId": "f34a2230-38bd-43e1-e122-772470cebea6"
      },
      "source": [
        "temp[temp<5776]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 829,  701,  797,  769,  733,  125,   97,  161,  193,  221, 2436, 2536,\n",
              "        2556, 2504, 2452, 2468, 2520, 2572, 2484, 3316, 3332, 3300, 3368, 3384,\n",
              "        3400, 3420, 3436, 3348, 4088, 4228, 4212, 4128, 4156, 4172, 4112, 4140,\n",
              "        4200, 4184, 5000, 4988, 4880, 4920, 4896, 4908, 4972, 4960, 4948, 4868,\n",
              "        4932], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wnz4C_89S-_",
        "outputId": "7e629e1f-be0a-4fe9-d912-6d5084e9fbfa"
      },
      "source": [
        "temp[torch.le(temp,5776)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 829,  701,  797,  733,  125,   97,  193,  221, 2436, 2536, 2556, 2504,\n",
              "        2452, 2468, 2520, 2572, 2484, 3316, 3332, 3300, 3368, 3384, 3400, 3420,\n",
              "        3436, 3348, 4088, 4228, 4212, 4128, 4156, 4172, 4112, 4140, 4200, 4184,\n",
              "        5000, 4988, 4880, 4920, 4896, 4908, 4972, 4960, 4948, 4868, 4932],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jeoeMEw9c6b",
        "outputId": "8065924d-4904-4cb5-a792-e6bfe78788dc"
      },
      "source": [
        "temp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 829,  701,  797, 6006,  733,  125,   97, 5778,  193,  221, 2436, 2536,\n",
              "        2556, 2504, 2452, 2468, 2520, 2572, 2484, 3316, 3332, 3300, 3368, 3384,\n",
              "        3400, 3420, 3436, 3348, 4088, 4228, 4212, 4128, 4156, 4172, 4112, 4140,\n",
              "        4200, 4184, 5000, 4988, 4880, 4920, 4896, 4908, 4972, 4960, 4948, 4868,\n",
              "        4932], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZxCUvnPfd8J",
        "outputId": "f4914eea-be09-41b3-e229-1c95409671c2"
      },
      "source": [
        "overlap.max(dim=0)[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0,  ..., 7, 7, 8], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2h6vtA-hV3O",
        "outputId": "049931d3-2da4-40de-e23b-3bff5deaf7a7"
      },
      "source": [
        "locs.size(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNb4GNN1iNMZ",
        "outputId": "6e3143f3-f956-4128-fbb3-220b38218c81"
      },
      "source": [
        "boxes[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6000, 0.5750, 0.8703, 0.7906]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OhQMo5CiSka"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahz775nm9Rz2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guMoMPx89SdI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}